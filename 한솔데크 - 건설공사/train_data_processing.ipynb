{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50eee59-b527-44e0-afa8-7e4ee190affb",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072d58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (0.3.35)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.18 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (0.3.19)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (0.3.8)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: anyio in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Using cached langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.17 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f5b93b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from torch) (2025.2.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, torch\n",
      "Successfully installed jinja2-3.1.5 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "493a074b-5e32-44c1-aee9-d378b1b37003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohyooseok/miniconda3/envs/lg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4223e-111b-49a8-a716-5f98caacf12f",
   "metadata": {},
   "source": [
    "# Data Load & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f871264a-ecd6-430d-ab1f-8edad746bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv', encoding = 'utf-8-sig')\n",
    "test = pd.read_csv('./test.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d57aff",
   "metadata": {},
   "source": [
    "##### train 데이터 결측값 채우기\n",
    "###### 1. 공사종류 + 공종을 기준으로 유사한 데이터를 찾아서 사고객체를 채우기\n",
    "###### 2. 공사종류 + 공종을 기준으로 유사한 데이터를 찾아서 작업프로세스를 채우기\n",
    "###### 3. 공사종류에 채운 사고객체 + 작업프로세스를 하여서 유사한 데이터를 찾아 공종을 채우기\n",
    "###### 4. 남은 결측값들에 대해서 공사종류만을 기준으로 사고객체 및 작업프로세스 채우기\n",
    "###### 5. 남은 결측값들에 대해서 공사종류만을 기준으로 공종 채우기\n",
    "###### 6. 마지막까지 채워지지 않은 결측값들에 대해서 특정 값으로 채우기\n",
    "###### 7. 사고원인 칼럼은 무조건 미상으로 채우지 않고, 재발방치대책의 키워드를 바탕으로 그 외의것은 미상으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc73a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계: 사고객체와 작업프로세스를 먼저 채우기\n",
    "# 공사종류 + 공종을 기준으로 사고객체 채우기\n",
    "grouped_object = train[train['사고객체'].notnull()].groupby(['공사종류', '공종'])['사고객체'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "train = train.merge(grouped_object, on=['공사종류', '공종'], how='left', suffixes=('', '_filled'))\n",
    "train['사고객체'] = train['사고객체'].fillna(train['사고객체_filled'])\n",
    "train.drop(columns=['사고객체_filled'], inplace=True)\n",
    "\n",
    "# 공사종류 + 공종을 기준으로 작업프로세스 채우기\n",
    "grouped_process = train[train['작업프로세스'].notnull()].groupby(['공사종류', '공종'])['작업프로세스'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "train = train.merge(grouped_process, on=['공사종류', '공종'], how='left', suffixes=('', '_filled'))\n",
    "train['작업프로세스'] = train['작업프로세스'].fillna(train['작업프로세스_filled'])\n",
    "train.drop(columns=['작업프로세스_filled'], inplace=True)\n",
    "\n",
    "# 2단계: 공종 채우기\n",
    "# 공사종류 + 사고객체 + 작업프로세스를 기준으로 공종을 채우기\n",
    "grouped_trade = train[train['공종'].notnull()].groupby(['공사종류', '사고객체', '작업프로세스'])['공종'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "train = train.merge(grouped_trade, on=['공사종류', '사고객체', '작업프로세스'], how='left', suffixes=('', '_filled'))\n",
    "train['공종'] = train['공종'].fillna(train['공종_filled'])\n",
    "train.drop(columns=['공종_filled'], inplace=True)\n",
    "\n",
    "# 3단계: 남은 결측값 처리\n",
    "# 공사종류만을 기준으로 사고객체 및 작업프로세스 채우기\n",
    "grouped_object_wo_trade = train[train['사고객체'].notnull()].groupby(['공사종류'])['사고객체'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "train = train.merge(grouped_object_wo_trade, on=['공사종류'], how='left', suffixes=('', '_alt'))\n",
    "train['사고객체'] = train['사고객체'].fillna(train['사고객체_alt'])\n",
    "train.drop(columns=['사고객체_alt'], inplace=True)\n",
    "\n",
    "grouped_process_wo_trade = train[train['작업프로세스'].notnull()].groupby(['공사종류'])['작업프로세스'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "train = train.merge(grouped_process_wo_trade, on=['공사종류'], how='left', suffixes=('', '_alt'))\n",
    "train['작업프로세스'] = train['작업프로세스'].fillna(train['작업프로세스_alt'])\n",
    "train.drop(columns=['작업프로세스_alt'], inplace=True)\n",
    "\n",
    "# 공사종류만을 기준으로 공종 채우기\n",
    "grouped_trade_wo_object = train[train['공종'].notnull()].groupby(['공사종류'])['공종'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "train = train.merge(grouped_trade_wo_object, on=['공사종류'], how='left', suffixes=('', '_alt'))\n",
    "train['공종'] = train['공종'].fillna(train['공종_alt'])\n",
    "train.drop(columns=['공종_alt'], inplace=True)\n",
    "\n",
    "# 4단계: 남아있는 특정 결측값을 직접 처리\n",
    "# ID: TRAIN_09122 (공사종류: 건축, 작업프로세스: 설치작업)\n",
    "train.loc[train['ID'] == 'TRAIN_09122', '공종'] = '건축 > 철근콘크리트공사'\n",
    "train.loc[train['ID'] == 'TRAIN_09122', '사고객체'] = '건설자재 > 철근'\n",
    "\n",
    "# ID: TRAIN_21617 (공사종류: 조경, 작업프로세스: 정리작업)\n",
    "train.loc[train['ID'] == 'TRAIN_21617', '사고객체'] = '기타 > 기타'\n",
    "\n",
    "# 3단계: 사고원인 자동 생성 (재발방지대책 기반 키워드 매칭)\n",
    "def infer_accident_cause(row):\n",
    "    prevention_text = str(row['재발방지대책 및 향후조치계획'])\n",
    "    \n",
    "    cause_mapping = {\n",
    "        '안전조치|안전장비|안전시설': '안전조치 미흡',\n",
    "        '부주의|주의 부족|주의 미흡': '작업 중 부주의',\n",
    "        '추락|낙하': '추락방지 미흡',\n",
    "        '낙석|낙하물': '낙하물 위험',\n",
    "        '전도': '전도 위험',\n",
    "        '미끄러짐|넘어짐': '미끄러짐',\n",
    "        '화재': '화재 위험',\n",
    "        '중량물|크레인': '중량물 작업 위험',\n",
    "        '전기|감전': '감전 위험'\n",
    "    }\n",
    "    \n",
    "    for pattern, cause in cause_mapping.items():\n",
    "        if any(keyword in prevention_text for keyword in pattern.split('|')):\n",
    "            return cause\n",
    "    \n",
    "    return '미상'  # 해당되지 않는 경우 미상으로 처리\n",
    "\n",
    "# 사고원인 결측값을 자동으로 채우기\n",
    "train['사고원인'] = train.apply(lambda row: infer_accident_cause(row) if pd.isnull(row['사고원인']) else row['사고원인'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9dccc",
   "metadata": {},
   "source": [
    "##### test 데이터 결측값 채우기\n",
    "###### 1. 공사종류 + 공종을 기준으로 유사한 데이터를 찾아서 사고객체를 채우기\n",
    "###### 2. 공사종류 + 공종을 기준으로 유사한 데이터를 찾아서 작업프로세스를 채우기\n",
    "###### 3. 공사종류에 채운 사고객체 + 작업프로세스를 하여서 유사한 데이터를 찾아 공종을 채우기\n",
    "###### 4. 남은 결측값들에 대해서 공사종류만을 기준으로 사고객체 및 작업프로세스 채우기\n",
    "###### 5. 남은 결측값들에 대해서 공사종류만을 기준으로 공종 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65652a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계: 사고객체와 작업프로세스를 먼저 채우기\n",
    "# 공사종류 + 공종을 기준으로 사고객체 채우기\n",
    "grouped_object = test[test['사고객체'].notnull()].groupby(['공사종류', '공종'])['사고객체'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "test = test.merge(grouped_object, on=['공사종류', '공종'], how='left', suffixes=('', '_filled'))\n",
    "test['사고객체'] = test['사고객체'].fillna(test['사고객체_filled'])\n",
    "test.drop(columns=['사고객체_filled'], inplace=True)\n",
    "\n",
    "# 공사종류 + 공종을 기준으로 작업프로세스 채우기\n",
    "grouped_process = test[test['작업프로세스'].notnull()].groupby(['공사종류', '공종'])['작업프로세스'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "test = test.merge(grouped_process, on=['공사종류', '공종'], how='left', suffixes=('', '_filled'))\n",
    "test['작업프로세스'] = test['작업프로세스'].fillna(test['작업프로세스_filled'])\n",
    "test.drop(columns=['작업프로세스_filled'], inplace=True)\n",
    "\n",
    "# 2단계: 공종 채우기\n",
    "# 공사종류 + 사고객체 + 작업프로세스를 기준으로 공종을 채우기\n",
    "grouped_trade = test[test['공종'].notnull()].groupby(['공사종류', '사고객체', '작업프로세스'])['공종'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "test = test.merge(grouped_trade, on=['공사종류', '사고객체', '작업프로세스'], how='left', suffixes=('', '_filled'))\n",
    "test['공종'] = test['공종'].fillna(test['공종_filled'])\n",
    "test.drop(columns=['공종_filled'], inplace=True)\n",
    "\n",
    "# 3단계: 남은 결측값 처리\n",
    "# 공사종류만을 기준으로 사고객체 및 작업프로세스 채우기\n",
    "grouped_object_wo_trade = test[test['사고객체'].notnull()].groupby(['공사종류'])['사고객체'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "test = test.merge(grouped_object_wo_trade, on=['공사종류'], how='left', suffixes=('', '_alt'))\n",
    "test['사고객체'] = test['사고객체'].fillna(test['사고객체_alt'])\n",
    "test.drop(columns=['사고객체_alt'], inplace=True)\n",
    "\n",
    "grouped_process_wo_trade = test[test['작업프로세스'].notnull()].groupby(['공사종류'])['작업프로세스'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "test = test.merge(grouped_process_wo_trade, on=['공사종류'], how='left', suffixes=('', '_alt'))\n",
    "test['작업프로세스'] = test['작업프로세스'].fillna(test['작업프로세스_alt'])\n",
    "test.drop(columns=['작업프로세스_alt'], inplace=True)\n",
    "\n",
    "# 공사종류만을 기준으로 공종 채우기\n",
    "grouped_trade_wo_object = test[test['공종'].notnull()].groupby(['공사종류'])['공종'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "test = test.merge(grouped_trade_wo_object, on=['공사종류'], how='left', suffixes=('', '_alt'))\n",
    "test['공종'] = test['공종'].fillna(test['공종_alt'])\n",
    "test.drop(columns=['공종_alt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcc0921-e9a7-407a-a070-148abb06aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "train['공사종류(대분류)'] = train['공사종류'].str.split(' / ').str[0]\n",
    "train['공사종류(중분류)'] = train['공사종류'].str.split(' / ').str[1]\n",
    "train['공종(대분류)'] = train['공종'].str.split(' > ').str[0]\n",
    "train['공종(중분류)'] = train['공종'].str.split(' > ').str[1]\n",
    "train['사고객체(대분류)'] = train['사고객체'].str.split(' > ').str[0]\n",
    "train['사고객체(중분류)'] = train['사고객체'].str.split(' > ').str[1]\n",
    "\n",
    "test['공사종류(대분류)'] = test['공사종류'].str.split(' / ').str[0]\n",
    "test['공사종류(중분류)'] = test['공사종류'].str.split(' / ').str[1]\n",
    "test['공종(대분류)'] = test['공종'].str.split(' > ').str[0]\n",
    "test['공종(중분류)'] = test['공종'].str.split(' > ').str[1]\n",
    "test['사고객체(대분류)'] = test['사고객체'].str.split(' > ').str[0]\n",
    "test['사고객체(중분류)'] = test['사고객체'].str.split(' > ').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e51bca-0c93-4412-9634-9f86ea9a36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_training_data = train.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"공사종류 대분류 '{row['공사종류(대분류)']}', 중분류 '{row['공사종류(중분류)']}' 공사 중 \"\n",
    "            f\"공종 대분류 '{row['공종(대분류)']}', 중분류 '{row['공종(중분류)']}' 작업에서 \"\n",
    "            f\"사고객체 '{row['사고객체(대분류)']}'(중분류: '{row['사고객체(중분류)']}')와 관련된 사고가 발생했습니다. \"\n",
    "            f\"작업 프로세스는 '{row['작업프로세스']}'이며, 사고 원인은 '{row['사고원인']}'입니다. \"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        ),\n",
    "        \"answer\": row[\"재발방지대책 및 향후조치계획\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_training_data = pd.DataFrame(list(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf53249-8aae-4308-a476-4200814da53d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 통합 생성\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"공사종류 대분류 '{row['공사종류(대분류)']}', 중분류 '{row['공사종류(중분류)']}' 공사 중 \"\n",
    "            f\"공종 대분류 '{row['공종(대분류)']}', 중분류 '{row['공종(중분류)']}' 작업에서 \"\n",
    "            f\"사고객체 '{row['사고객체(대분류)']}'(중분류: '{row['사고객체(중분류)']}')와 관련된 사고가 발생했습니다. \"\n",
    "            f\"작업 프로세스는 '{row['작업프로세스']}'이며, 사고 원인은 '{row['사고원인']}'입니다. \"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bea209-6521-4974-a785-c2faf5af427c",
   "metadata": {},
   "source": [
    "# Model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f751a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/TimDettmers/bitsandbytes.git\n",
      "  Cloning https://github.com/TimDettmers/bitsandbytes.git to /private/var/folders/8r/jhvmt0q91s7gtq_25g2ysrwr0000gn/T/pip-req-build-simsc3jz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/TimDettmers/bitsandbytes.git /private/var/folders/8r/jhvmt0q91s7gtq_25g2ysrwr0000gn/T/pip-req-build-simsc3jz\n",
      "  Resolved https://github.com/TimDettmers/bitsandbytes.git to commit 86b6c37a8ad448230cedb60753f63150b603a112\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch<3,>=2.0 (from bitsandbytes==0.45.3.dev0)\n",
      "  Using cached torch-2.6.0-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting numpy>=1.17 (from bitsandbytes==0.45.3.dev0)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting filelock (from torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.3.dev0)\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Using cached torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: bitsandbytes\n",
      "  Building wheel for bitsandbytes (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bitsandbytes: filename=bitsandbytes-0.45.3.dev0-cp39-cp39-macosx_11_0_arm64.whl size=90539 sha256=c2bd2f68ef2ae91547eae82eff5ec6bdf02f5a2495c4d2f547d3078f00c7689e\n",
      "  Stored in directory: /private/var/folders/8r/jhvmt0q91s7gtq_25g2ysrwr0000gn/T/pip-ephem-wheel-cache-hwk5lf1v/wheels/87/78/9b/9b08fcbebb2b3aacfd4d6c1513cfe77c5eaf6cfa22a49eda3c\n",
      "Successfully built bitsandbytes\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, bitsandbytes\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Uninstalling networkx-3.2.1:\n",
      "      Successfully uninstalled networkx-3.2.1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.17.0\n",
      "    Uninstalling filelock-3.17.0:\n",
      "      Successfully uninstalled filelock-3.17.0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.5\n",
      "    Uninstalling Jinja2-3.1.5:\n",
      "      Successfully uninstalled Jinja2-3.1.5\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
      "langchain-community 0.3.17 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 bitsandbytes-0.45.3.dev0 filelock-3.17.0 fsspec-2025.2.0 jinja2-3.1.5 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 sympy-1.13.1 torch-2.6.0 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall git+https://github.com/TimDettmers/bitsandbytes.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b1d4ec-a335-4a3e-803e-1ee0a8e8b482",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnf4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lg/lib/python3.9/site-packages/transformers/utils/quantization_config.py:412\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bnb_4bit_compute_dtype, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, bnb_4bit_compute_dtype)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bnb_4bit_compute_dtype, \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m bnb_4bit_compute_dtype\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fd9f2e5-4293-48b1-9c8a-347cf500b257",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNCSOFT/Llama-VARCO-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_id, quantization_config\u001b[38;5;241m=\u001b[39mbnb_config, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lg/lib/python3.9/site-packages/transformers/utils/import_utils.py:1736\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1736\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lg/lib/python3.9/site-packages/transformers/utils/import_utils.py:1724\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1722\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ff667",
   "metadata": {},
   "source": [
    "# Vector store 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b47aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터 준비\n",
    "train_questions_prevention = combined_training_data['question'].tolist()\n",
    "train_answers_prevention = combined_training_data['answer'].tolist()\n",
    "\n",
    "train_documents = [\n",
    "    f\"Q: {q1}\\nA: {a1}\" \n",
    "    for q1, a1 in zip(train_questions_prevention, train_answers_prevention)\n",
    "]\n",
    "\n",
    "# 임베딩 생성\n",
    "embedding_model_name = \"jhgan/ko-sbert-nli\"  # 임베딩 모델 선택\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 벡터 스토어에 문서 추가\n",
    "vector_store = FAISS.from_texts(train_documents, embedding)\n",
    "\n",
    "# Retriever 정의\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533fc9f",
   "metadata": {},
   "source": [
    "# RAG chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d4e9e-fd99-4157-abab-a367a24fa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,  # sampling 활성화\n",
    "    temperature=0.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=64,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### 지침: 당신은 건설 안전 전문가입니다.\n",
    "질문에 대한 답변을 핵심 내용만 요약하여 간략하게 작성하세요.\n",
    "- 서론, 배경 설명 또는 추가 설명을 절대 포함하지 마세요.\n",
    "- 다음과 같은 조치를 취할 것을 제안합니다: 와 같은 내용을 포함하지 마세요.\n",
    "\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# 커스텀 프롬프트 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "\n",
    "# RAG 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  # 단순 컨텍스트 결합 방식 사용\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}  # 커스텀 프롬프트 적용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b3640-3968-4c2b-a7d4-30f586fadd66",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590331a4-75f0-4012-8f45-8f2652772c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 실행 및 결과 저장\n",
    "test_results = []\n",
    "\n",
    "print(\"테스트 실행 시작... 총 테스트 샘플 수:\", len(combined_test_data))\n",
    "\n",
    "for idx, row in combined_test_data.iterrows():\n",
    "    # 50개당 한 번 진행 상황 출력\n",
    "    if (idx + 1) % 50 == 0 or idx == 0:\n",
    "        print(f\"\\n[샘플 {idx + 1}/{len(combined_test_data)}] 진행 중...\")\n",
    "\n",
    "    # RAG 체인 호출 및 결과 생성\n",
    "    prevention_result = qa_chain.invoke(row['question'])\n",
    "\n",
    "    # 결과 저장\n",
    "    result_text = prevention_result['result']\n",
    "    test_results.append(result_text)\n",
    "\n",
    "print(\"\\n테스트 실행 완료! 총 결과 수:\", len(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9022911",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# 문장 리스트를 입력하여 임베딩 생성\n",
    "pred_embeddings = embedding.encode(test_results)\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae869742-4a0b-45bc-8a50-e385b67a9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "# 최종 결과 저장\n",
    "submission.iloc[:,1] = test_results\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "\n",
    "# 최종 결과를 CSV로 저장\n",
    "submission.to_csv('./baseline_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
